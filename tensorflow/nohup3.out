2018-04-10 22:18:21,543 - brc - INFO - Running with args : Namespace(algo='BIDAF', batch_size=32, brc_dir='../data/baidu', dev_files=['../data/preprocessed/devset/search.dev.json', '../data/preprocessed/devset/zhidao.dev.json'], dropout_keep_prob=1, embed_size=300, epochs=10, evaluate=False, gpu='0', hidden_size=150, learning_rate=0.001, log_path=None, max_a_len=200, max_p_len=500, max_p_num=5, max_q_len=60, model_dir='../data/models/', optim='adam', predict=False, prepare=True, result_dir='../data/results/', summary_dir='../data/summary/', test_files=['../data/preprocessed/testset/search.test1.json', '../data/preprocessed/testset/zhidao.test1.json'], train=False, train_files=['../data/preprocessed/trainset/search.train.json', '../data/preprocessed/trainset/zhidao.train.json'], vocab_dir='../data/vocab/', weight_decay=0)
2018-04-10 22:18:21,543 - brc - INFO - Checking the data files...
2018-04-10 22:18:21,543 - brc - INFO - Preparing the directories...
2018-04-10 22:18:21,543 - brc - INFO - Building vocabulary...
2018-04-10 22:18:51,276 - brc - INFO - Train set size: 56699 questions.
2018-04-10 22:19:03,820 - brc - INFO - Dev set size: 10000 questions.
2018-04-10 22:20:02,896 - brc - INFO - Test set size: 60000 questions.
2018-04-10 22:20:33,229 - brc - INFO - After filter 155916 tokens, the final vocab size is 245582
2018-04-10 22:20:33,229 - brc - INFO - Assigning embeddings...
2018-04-10 22:20:33,807 - brc - INFO - Saving vocab...
2018-04-10 22:20:34,906 - brc - INFO - Done with preparing!
2018-04-10 22:20:40,783 - brc - INFO - Running with args : Namespace(algo='BIDAF', batch_size=32, brc_dir='../data/baidu', dev_files=['../data/preprocessed/devset/search.dev.json', '../data/preprocessed/devset/zhidao.dev.json'], dropout_keep_prob=1, embed_size=300, epochs=10, evaluate=False, gpu='2', hidden_size=150, learning_rate=0.001, log_path=None, max_a_len=200, max_p_len=500, max_p_num=5, max_q_len=60, model_dir='../data/models/', optim='adam', predict=False, prepare=False, result_dir='../data/results/', summary_dir='../data/summary/', test_files=['../data/preprocessed/testset/search.test1.json', '../data/preprocessed/testset/zhidao.test1.json'], train=True, train_files=['../data/preprocessed/trainset/search.train.json', '../data/preprocessed/trainset/zhidao.train.json'], vocab_dir='../data/vocab/', weight_decay=0)
2018-04-10 22:20:40,783 - brc - INFO - Load data_set and vocab...
2018-04-10 22:21:09,010 - brc - INFO - Train set size: 56699 questions.
2018-04-10 22:21:21,496 - brc - INFO - Dev set size: 10000 questions.
2018-04-10 22:21:21,496 - brc - INFO - Converting text into ids...
2018-04-10 22:21:39,098 - brc - INFO - Initialize the model...
2018-04-10 22:21:42,913 - brc - INFO - Time to build graph: 2.1375415325164795 s
2018-04-10 22:21:48,596 - brc - INFO - There are 77168403 parameters in the model
2018-04-10 22:21:52,053 - brc - INFO - Training the model...
2018-04-10 22:21:52,054 - brc - INFO - Training the model for epoch 1
2018-04-10 22:22:56,405 - brc - INFO - Average loss from batch 1 to 50 is 9.994008464813232
2018-04-10 22:24:02,585 - brc - INFO - Average loss from batch 51 to 100 is 8.728060207366944
2018-04-10 22:25:09,103 - brc - INFO - Average loss from batch 101 to 150 is 8.56874792098999
2018-04-10 22:26:15,349 - brc - INFO - Average loss from batch 151 to 200 is 8.56911961555481
2018-04-10 22:27:21,794 - brc - INFO - Average loss from batch 201 to 250 is 8.266615018844604
2018-04-10 22:28:28,843 - brc - INFO - Average loss from batch 251 to 300 is 8.120470695495605
2018-04-10 22:29:34,233 - brc - INFO - Average loss from batch 301 to 350 is 8.047433710098266
2018-04-10 22:30:40,616 - brc - INFO - Average loss from batch 351 to 400 is 8.106932802200317
2018-04-10 22:31:48,563 - brc - INFO - Average loss from batch 401 to 450 is 7.942757005691528
2018-04-10 22:32:54,928 - brc - INFO - Average loss from batch 451 to 500 is 8.052242698669433
2018-04-10 22:33:59,719 - brc - INFO - Average loss from batch 501 to 550 is 7.860729675292969
2018-04-10 22:35:06,001 - brc - INFO - Average loss from batch 551 to 600 is 7.929898653030396
2018-04-10 22:36:13,362 - brc - INFO - Average loss from batch 601 to 650 is 7.836054124832153
2018-04-10 22:37:19,546 - brc - INFO - Average loss from batch 651 to 700 is 7.808634176254272
2018-04-10 22:38:25,516 - brc - INFO - Average loss from batch 701 to 750 is 7.738894968032837
2018-04-10 22:39:31,400 - brc - INFO - Average loss from batch 751 to 800 is 7.656552534103394
2018-04-10 22:40:37,342 - brc - INFO - Average loss from batch 801 to 850 is 7.5747388648986815
2018-04-10 22:41:43,000 - brc - INFO - Average loss from batch 851 to 900 is 7.869227952957154
2018-04-10 22:42:48,893 - brc - INFO - Average loss from batch 901 to 950 is 7.571081352233887
2018-04-10 22:43:54,548 - brc - INFO - Average loss from batch 951 to 1000 is 7.724175834655762
2018-04-10 22:45:00,404 - brc - INFO - Average loss from batch 1001 to 1050 is 7.830273494720459
2018-04-10 22:46:06,074 - brc - INFO - Average loss from batch 1051 to 1100 is 7.62375617980957
2018-04-10 22:47:10,617 - brc - INFO - Average loss from batch 1101 to 1150 is 7.572236394882202
2018-04-10 22:48:17,064 - brc - INFO - Average loss from batch 1151 to 1200 is 7.535858821868897
2018-04-10 22:49:23,326 - brc - INFO - Average loss from batch 1201 to 1250 is 7.566130723953247
2018-04-10 22:50:29,896 - brc - INFO - Average loss from batch 1251 to 1300 is 7.478699712753296
2018-04-10 22:51:35,922 - brc - INFO - Average loss from batch 1301 to 1350 is 7.45870156288147
2018-04-10 22:52:41,881 - brc - INFO - Average loss from batch 1351 to 1400 is 7.573257427215577
2018-04-10 22:53:48,003 - brc - INFO - Average loss from batch 1401 to 1450 is 7.599316282272339
2018-04-10 22:54:52,833 - brc - INFO - Average loss from batch 1451 to 1500 is 7.538585863113403
2018-04-10 22:56:00,477 - brc - INFO - Average loss from batch 1501 to 1550 is 7.546860046386719
2018-04-10 22:57:06,687 - brc - INFO - Average loss from batch 1551 to 1600 is 7.370631370544434
2018-04-10 22:58:12,713 - brc - INFO - Average loss from batch 1601 to 1650 is 7.403528366088867
2018-04-10 22:59:18,820 - brc - INFO - Average loss from batch 1651 to 1700 is 7.3911400985717775
2018-04-10 23:00:24,898 - brc - INFO - Average loss from batch 1701 to 1750 is 7.585033750534057
2018-04-10 23:00:54,060 - brc - INFO - Average train loss for epoch 1 is 7.8523300366144415
2018-04-10 23:00:54,060 - brc - INFO - Evaluating the model after epoch 1
2018-04-10 23:06:03,783 - brc - INFO - Dev eval loss 12.70661145172119
2018-04-10 23:06:03,783 - brc - INFO - Dev eval result: {'Bleu-1': 0.40309232181963395, 'Bleu-2': 0.3354158276084785, 'Bleu-3': 0.2956716436334265, 'Bleu-4': 0.27024359107862944, 'Rouge-L': 0.34383783531015139}
2018-04-10 23:06:05,799 - brc - INFO - Model saved in ../data/models/, with prefix BIDAF.
2018-04-10 23:06:05,800 - brc - INFO - Training the model for epoch 2
2018-04-10 23:07:10,818 - brc - INFO - Average loss from batch 1 to 50 is 7.126543254852295
2018-04-10 23:08:16,304 - brc - INFO - Average loss from batch 51 to 100 is 7.087732677459717
2018-04-10 23:09:21,641 - brc - INFO - Average loss from batch 101 to 150 is 6.983200874328613
2018-04-10 23:10:26,864 - brc - INFO - Average loss from batch 151 to 200 is 6.9742169857025145
2018-04-10 23:11:31,800 - brc - INFO - Average loss from batch 201 to 250 is 6.93929235458374
2018-04-10 23:12:36,931 - brc - INFO - Average loss from batch 251 to 300 is 7.075315055847168
2018-04-10 23:13:40,444 - brc - INFO - Average loss from batch 301 to 350 is 7.184048166275025
2018-04-10 23:14:45,305 - brc - INFO - Average loss from batch 351 to 400 is 6.791126403808594
2018-04-10 23:15:50,385 - brc - INFO - Average loss from batch 401 to 450 is 7.017139167785644
2018-04-10 23:16:55,755 - brc - INFO - Average loss from batch 451 to 500 is 7.14098822593689
2018-04-10 23:18:01,129 - brc - INFO - Average loss from batch 501 to 550 is 6.999612588882446
2018-04-10 23:19:06,599 - brc - INFO - Average loss from batch 551 to 600 is 7.03527452468872
2018-04-10 23:20:11,885 - brc - INFO - Average loss from batch 601 to 650 is 6.864136390686035
2018-04-10 23:21:17,310 - brc - INFO - Average loss from batch 651 to 700 is 6.8062540054321286
2018-04-10 23:22:22,889 - brc - INFO - Average loss from batch 701 to 750 is 6.941438817977906
2018-04-10 23:23:28,489 - brc - INFO - Average loss from batch 751 to 800 is 6.943804931640625
2018-04-10 23:24:33,977 - brc - INFO - Average loss from batch 801 to 850 is 6.654807186126709
2018-04-10 23:25:39,430 - brc - INFO - Average loss from batch 851 to 900 is 6.9936191368103025
2018-04-10 23:26:45,025 - brc - INFO - Average loss from batch 901 to 950 is 6.841503086090088
2018-04-10 23:27:50,462 - brc - INFO - Average loss from batch 951 to 1000 is 6.836070966720581
2018-04-10 23:28:54,574 - brc - INFO - Average loss from batch 1001 to 1050 is 6.878347654342651
2018-04-10 23:29:59,876 - brc - INFO - Average loss from batch 1051 to 1100 is 7.00233286857605
2018-04-10 23:31:09,799 - brc - INFO - Average loss from batch 1101 to 1150 is 6.856398735046387
2018-04-10 23:32:50,422 - brc - INFO - Average loss from batch 1151 to 1200 is 7.030752048492432
2018-04-10 23:34:14,593 - brc - INFO - Average loss from batch 1201 to 1250 is 6.913251676559448
2018-04-10 23:35:19,372 - brc - INFO - Average loss from batch 1251 to 1300 is 6.864724769592285
2018-04-10 23:36:24,422 - brc - INFO - Average loss from batch 1301 to 1350 is 6.92199857711792
2018-04-10 23:37:28,642 - brc - INFO - Average loss from batch 1351 to 1400 is 7.070981607437134
2018-04-10 23:38:32,986 - brc - INFO - Average loss from batch 1401 to 1450 is 6.774833822250367
2018-04-10 23:39:35,947 - brc - INFO - Average loss from batch 1451 to 1500 is 6.985704364776612
2018-04-10 23:40:40,375 - brc - INFO - Average loss from batch 1501 to 1550 is 7.109050731658936
2018-04-10 23:41:44,892 - brc - INFO - Average loss from batch 1551 to 1600 is 6.9992907524108885
2018-04-10 23:42:48,996 - brc - INFO - Average loss from batch 1601 to 1650 is 6.853802385330201
2018-04-10 23:43:53,124 - brc - INFO - Average loss from batch 1651 to 1700 is 6.881400661468506
2018-04-10 23:44:57,165 - brc - INFO - Average loss from batch 1701 to 1750 is 7.014978647232056
2018-04-10 23:45:25,582 - brc - INFO - Average train loss for epoch 2 is 6.952834916624872
2018-04-10 23:45:25,583 - brc - INFO - Evaluating the model after epoch 2
2018-04-10 23:53:09,806 - brc - INFO - Dev eval loss 12.993387455749511
2018-04-10 23:53:09,806 - brc - INFO - Dev eval result: {'Bleu-1': 0.4982194208792889, 'Bleu-2': 0.4188763162510996, 'Bleu-3': 0.372406226253392, 'Bleu-4': 0.3424120589823242, 'Rouge-L': 0.37101418869658664}
2018-04-10 23:53:13,425 - brc - INFO - Model saved in ../data/models/, with prefix BIDAF.
2018-04-10 23:53:13,425 - brc - INFO - Training the model for epoch 3
2018-04-10 23:54:37,170 - brc - INFO - Average loss from batch 1 to 50 is 5.952961120605469
2018-04-10 23:56:03,074 - brc - INFO - Average loss from batch 51 to 100 is 6.118428573608399
2018-04-10 23:57:26,013 - brc - INFO - Average loss from batch 101 to 150 is 5.932049121856689
2018-04-10 23:58:50,670 - brc - INFO - Average loss from batch 151 to 200 is 6.017293310165405
2018-04-10 23:59:54,983 - brc - INFO - Average loss from batch 201 to 250 is 5.986661033630371
2018-04-11 00:00:59,762 - brc - INFO - Average loss from batch 251 to 300 is 5.847318239212036
2018-04-11 00:02:04,465 - brc - INFO - Average loss from batch 301 to 350 is 5.84257134437561
2018-04-11 00:03:09,132 - brc - INFO - Average loss from batch 351 to 400 is 5.837155017852783
2018-04-11 00:04:13,797 - brc - INFO - Average loss from batch 401 to 450 is 5.754054088592529
2018-04-11 00:05:18,174 - brc - INFO - Average loss from batch 451 to 500 is 5.943203659057617
2018-04-11 00:06:22,636 - brc - INFO - Average loss from batch 501 to 550 is 6.0654786109924315
2018-04-11 00:07:26,026 - brc - INFO - Average loss from batch 551 to 600 is 6.0199294090271
2018-04-11 00:08:30,727 - brc - INFO - Average loss from batch 601 to 650 is 6.007266054153442
2018-04-11 00:09:35,381 - brc - INFO - Average loss from batch 651 to 700 is 5.942636766433716
2018-04-11 00:10:38,671 - brc - INFO - Average loss from batch 701 to 750 is 5.874936561584473
2018-04-11 00:11:44,563 - brc - INFO - Average loss from batch 751 to 800 is 6.187236738204956
2018-04-11 00:12:49,358 - brc - INFO - Average loss from batch 801 to 850 is 6.024827222824097
2018-04-11 00:13:54,119 - brc - INFO - Average loss from batch 851 to 900 is 5.741981325149536
2018-04-11 00:14:58,503 - brc - INFO - Average loss from batch 901 to 950 is 6.214133577346802
2018-04-11 00:16:02,769 - brc - INFO - Average loss from batch 951 to 1000 is 5.787899551391601
2018-04-11 00:17:07,106 - brc - INFO - Average loss from batch 1001 to 1050 is 6.0691964817047115
2018-04-11 00:18:09,937 - brc - INFO - Average loss from batch 1051 to 1100 is 5.922034769058228
2018-04-11 00:19:14,160 - brc - INFO - Average loss from batch 1101 to 1150 is 5.904660673141479
2018-04-11 00:20:18,329 - brc - INFO - Average loss from batch 1151 to 1200 is 5.9769469738006595
2018-04-11 00:21:22,378 - brc - INFO - Average loss from batch 1201 to 1250 is 6.079156675338745
2018-04-11 00:22:26,410 - brc - INFO - Average loss from batch 1251 to 1300 is 6.077155303955078
2018-04-11 00:23:30,382 - brc - INFO - Average loss from batch 1301 to 1350 is 6.029376764297485
2018-04-11 00:24:34,313 - brc - INFO - Average loss from batch 1351 to 1400 is 6.088339471817017
2018-04-11 00:25:38,508 - brc - INFO - Average loss from batch 1401 to 1450 is 6.101220359802246
2018-04-11 00:26:42,533 - brc - INFO - Average loss from batch 1451 to 1500 is 5.919802684783935
2018-04-11 00:27:45,354 - brc - INFO - Average loss from batch 1501 to 1550 is 6.005099611282349
2018-04-11 00:28:49,562 - brc - INFO - Average loss from batch 1551 to 1600 is 6.134865970611572
2018-04-11 00:29:53,998 - brc - INFO - Average loss from batch 1601 to 1650 is 6.023493404388428
2018-04-11 00:30:58,652 - brc - INFO - Average loss from batch 1651 to 1700 is 5.98707760810852
2018-04-11 00:32:04,237 - brc - INFO - Average loss from batch 1701 to 1750 is 5.958604116439819
2018-04-11 00:32:33,216 - brc - INFO - Average train loss for epoch 3 is 5.983143491974924
2018-04-11 00:32:33,216 - brc - INFO - Evaluating the model after epoch 3
2018-04-11 00:37:37,097 - brc - INFO - Dev eval loss 13.618321737670898
2018-04-11 00:37:37,097 - brc - INFO - Dev eval result: {'Bleu-1': 0.3711357392923428, 'Bleu-2': 0.31050874125807887, 'Bleu-3': 0.2745869062475742, 'Bleu-4': 0.25142702591497, 'Rouge-L': 0.32990895584060392}
2018-04-11 00:37:37,097 - brc - INFO - Training the model for epoch 4
2018-04-11 00:38:44,627 - brc - INFO - Average loss from batch 1 to 50 is 4.591804285049438
2018-04-11 00:39:51,895 - brc - INFO - Average loss from batch 51 to 100 is 4.441288361549377
2018-04-11 00:40:58,984 - brc - INFO - Average loss from batch 101 to 150 is 4.666989288330078
2018-04-11 00:42:05,768 - brc - INFO - Average loss from batch 151 to 200 is 4.649481801986695
2018-04-11 00:43:12,685 - brc - INFO - Average loss from batch 201 to 250 is 4.50313157081604
2018-04-11 00:44:20,013 - brc - INFO - Average loss from batch 251 to 300 is 4.699178605079651
2018-04-11 00:45:27,287 - brc - INFO - Average loss from batch 301 to 350 is 4.653343596458435
2018-04-11 00:46:34,605 - brc - INFO - Average loss from batch 351 to 400 is 4.654154992103576
2018-04-11 00:47:42,102 - brc - INFO - Average loss from batch 401 to 450 is 4.471661286354065
2018-04-11 00:48:49,333 - brc - INFO - Average loss from batch 451 to 500 is 4.688051505088806
2018-04-11 00:49:56,432 - brc - INFO - Average loss from batch 501 to 550 is 4.6804172849655155
2018-04-11 00:51:03,498 - brc - INFO - Average loss from batch 551 to 600 is 4.8094480609893795
2018-04-11 00:52:09,870 - brc - INFO - Average loss from batch 601 to 650 is 4.804090132713318
2018-04-11 00:53:16,716 - brc - INFO - Average loss from batch 651 to 700 is 4.724710340499878
2018-04-11 00:54:23,443 - brc - INFO - Average loss from batch 701 to 750 is 4.863399453163147
2018-04-11 00:55:30,419 - brc - INFO - Average loss from batch 751 to 800 is 4.80499391078949
2018-04-11 00:56:36,768 - brc - INFO - Average loss from batch 801 to 850 is 4.824857649803161
2018-04-11 00:57:42,988 - brc - INFO - Average loss from batch 851 to 900 is 4.858626775741577
2018-04-11 00:58:49,504 - brc - INFO - Average loss from batch 901 to 950 is 4.745018854141235
2018-04-11 00:59:54,896 - brc - INFO - Average loss from batch 951 to 1000 is 4.806566162109375
2018-04-11 01:01:01,752 - brc - INFO - Average loss from batch 1001 to 1050 is 4.8328944683074955
2018-04-11 01:02:08,838 - brc - INFO - Average loss from batch 1051 to 1100 is 4.919011006355285
2018-04-11 01:03:16,079 - brc - INFO - Average loss from batch 1101 to 1150 is 4.931552901268005
2018-04-11 01:04:21,747 - brc - INFO - Average loss from batch 1151 to 1200 is 4.720901384353637
2018-04-11 01:05:29,340 - brc - INFO - Average loss from batch 1201 to 1250 is 4.980728569030762
2018-04-11 01:06:35,236 - brc - INFO - Average loss from batch 1251 to 1300 is 5.079063816070557
2018-04-11 01:07:41,367 - brc - INFO - Average loss from batch 1301 to 1350 is 4.83946035861969
2018-04-11 01:08:47,528 - brc - INFO - Average loss from batch 1351 to 1400 is 4.892874536514282
2018-04-11 01:09:53,649 - brc - INFO - Average loss from batch 1401 to 1450 is 4.902587084770203
2018-04-11 01:10:59,819 - brc - INFO - Average loss from batch 1451 to 1500 is 4.972519884109497
2018-04-11 01:12:05,723 - brc - INFO - Average loss from batch 1501 to 1550 is 5.157135276794434
2018-04-11 01:13:11,697 - brc - INFO - Average loss from batch 1551 to 1600 is 4.954520092010498
2018-04-11 01:14:17,521 - brc - INFO - Average loss from batch 1601 to 1650 is 5.036124210357666
2018-04-11 01:15:23,428 - brc - INFO - Average loss from batch 1651 to 1700 is 4.98097677230835
2018-04-11 01:16:29,473 - brc - INFO - Average loss from batch 1701 to 1750 is 5.029813318252564
2018-04-11 01:16:58,617 - brc - INFO - Average train loss for epoch 4 is 4.806578890007701
2018-04-11 01:16:58,617 - brc - INFO - Evaluating the model after epoch 4
2018-04-11 01:22:20,130 - brc - INFO - Dev eval loss 15.666952709960938
2018-04-11 01:22:20,130 - brc - INFO - Dev eval result: {'Bleu-1': 0.45046570220167165, 'Bleu-2': 0.37492071736630056, 'Bleu-3': 0.3307997228907525, 'Bleu-4': 0.30254629696035795, 'Rouge-L': 0.33585214986311562}
2018-04-11 01:22:20,130 - brc - INFO - Training the model for epoch 5
2018-04-11 01:23:25,192 - brc - INFO - Average loss from batch 1 to 50 is 3.323489980697632
2018-04-11 01:24:30,846 - brc - INFO - Average loss from batch 51 to 100 is 3.386090440750122
2018-04-11 01:25:36,620 - brc - INFO - Average loss from batch 101 to 150 is 3.476925296783447
2018-04-11 01:26:42,601 - brc - INFO - Average loss from batch 151 to 200 is 3.3664145040512086
2018-04-11 01:27:48,412 - brc - INFO - Average loss from batch 201 to 250 is 3.4473619294166564
2018-04-11 01:28:54,057 - brc - INFO - Average loss from batch 251 to 300 is 3.523559446334839
2018-04-11 01:29:59,907 - brc - INFO - Average loss from batch 301 to 350 is 3.54979238986969
2018-04-11 01:31:05,449 - brc - INFO - Average loss from batch 351 to 400 is 3.596545624732971
2018-04-11 01:32:11,469 - brc - INFO - Average loss from batch 401 to 450 is 3.659760046005249
2018-04-11 01:33:17,381 - brc - INFO - Average loss from batch 451 to 500 is 3.371845235824585
2018-04-11 01:34:23,450 - brc - INFO - Average loss from batch 501 to 550 is 3.6604853105545043
2018-04-11 01:35:29,335 - brc - INFO - Average loss from batch 551 to 600 is 3.784753541946411
2018-04-11 01:36:34,158 - brc - INFO - Average loss from batch 601 to 650 is 3.6199145555496215
2018-04-11 01:37:40,086 - brc - INFO - Average loss from batch 651 to 700 is 3.4597893381118774
2018-04-11 01:38:45,938 - brc - INFO - Average loss from batch 701 to 750 is 3.7253532123565676
2018-04-11 01:39:51,901 - brc - INFO - Average loss from batch 751 to 800 is 3.8232180738449095
2018-04-11 01:40:58,112 - brc - INFO - Average loss from batch 801 to 850 is 3.7002464962005615
2018-04-11 01:42:04,212 - brc - INFO - Average loss from batch 851 to 900 is 3.7955241870880125
2018-04-11 01:43:10,049 - brc - INFO - Average loss from batch 901 to 950 is 3.8255429649353028
2018-04-11 01:44:15,886 - brc - INFO - Average loss from batch 951 to 1000 is 3.7526942014694216
2018-04-11 01:45:22,076 - brc - INFO - Average loss from batch 1001 to 1050 is 3.77422905921936
2018-04-11 01:46:28,024 - brc - INFO - Average loss from batch 1051 to 1100 is 3.8474760675430297
2018-04-11 01:47:33,759 - brc - INFO - Average loss from batch 1101 to 1150 is 3.9049231672286986
2018-04-11 01:48:39,790 - brc - INFO - Average loss from batch 1151 to 1200 is 3.909701776504517
2018-04-11 01:49:45,862 - brc - INFO - Average loss from batch 1201 to 1250 is 3.915005350112915
2018-04-11 01:50:51,661 - brc - INFO - Average loss from batch 1251 to 1300 is 3.6923717355728147
2018-04-11 01:51:57,181 - brc - INFO - Average loss from batch 1301 to 1350 is 4.03845061302185
2018-04-11 01:53:02,951 - brc - INFO - Average loss from batch 1351 to 1400 is 3.809663486480713
2018-04-11 01:54:08,786 - brc - INFO - Average loss from batch 1401 to 1450 is 4.01161907196045
2018-04-11 01:55:14,541 - brc - INFO - Average loss from batch 1451 to 1500 is 3.904904456138611
2018-04-11 01:56:18,201 - brc - INFO - Average loss from batch 1501 to 1550 is 3.969335422515869
2018-04-11 01:57:23,001 - brc - INFO - Average loss from batch 1551 to 1600 is 4.042399873733521
2018-04-11 01:58:27,828 - brc - INFO - Average loss from batch 1601 to 1650 is 3.9337170124053955
2018-04-11 01:59:32,472 - brc - INFO - Average loss from batch 1651 to 1700 is 4.023795175552368
2018-04-11 02:00:37,245 - brc - INFO - Average loss from batch 1701 to 1750 is 4.030720663070679
2018-04-11 02:01:05,979 - brc - INFO - Average train loss for epoch 5 is 3.7377194311150412
2018-04-11 02:01:05,979 - brc - INFO - Evaluating the model after epoch 5
2018-04-11 02:06:24,354 - brc - INFO - Dev eval loss 16.60564621887207
2018-04-11 02:06:24,354 - brc - INFO - Dev eval result: {'Bleu-1': 0.4540906983938217, 'Bleu-2': 0.37721613682853733, 'Bleu-3': 0.3326331167279978, 'Bleu-4': 0.3041504956538137, 'Rouge-L': 0.3386413271951112}
2018-04-11 02:06:24,354 - brc - INFO - Training the model for epoch 6
2018-04-11 02:07:28,914 - brc - INFO - Average loss from batch 1 to 50 is 2.52773939371109
2018-04-11 02:08:33,788 - brc - INFO - Average loss from batch 51 to 100 is 2.5748122787475585
2018-04-11 02:09:38,911 - brc - INFO - Average loss from batch 101 to 150 is 2.577213978767395
2018-04-11 02:10:44,066 - brc - INFO - Average loss from batch 151 to 200 is 2.5317187118530273
2018-04-11 02:11:49,252 - brc - INFO - Average loss from batch 201 to 250 is 2.753766598701477
2018-04-11 02:12:54,287 - brc - INFO - Average loss from batch 251 to 300 is 2.6017791986465455
2018-04-11 02:13:59,252 - brc - INFO - Average loss from batch 301 to 350 is 2.660030388832092
2018-04-11 02:15:04,181 - brc - INFO - Average loss from batch 351 to 400 is 2.768257565498352
2018-04-11 02:16:09,338 - brc - INFO - Average loss from batch 401 to 450 is 2.803840863704681
2018-04-11 02:17:14,474 - brc - INFO - Average loss from batch 451 to 500 is 2.7928821086883544
2018-04-11 02:18:18,408 - brc - INFO - Average loss from batch 501 to 550 is 2.531267857551575
2018-04-11 02:19:23,688 - brc - INFO - Average loss from batch 551 to 600 is 2.9652327346801757
2018-04-11 02:20:28,853 - brc - INFO - Average loss from batch 601 to 650 is 2.7328454542160032
2018-04-11 02:21:33,937 - brc - INFO - Average loss from batch 651 to 700 is 3.0127655768394472
2018-04-11 02:22:38,846 - brc - INFO - Average loss from batch 701 to 750 is 2.8056835603713988
2018-04-11 02:23:43,735 - brc - INFO - Average loss from batch 751 to 800 is 2.847029297351837
2018-04-11 02:24:47,424 - brc - INFO - Average loss from batch 801 to 850 is 2.9624766635894777
2018-04-11 02:25:53,588 - brc - INFO - Average loss from batch 851 to 900 is 2.8354576230049133
2018-04-11 02:26:58,177 - brc - INFO - Average loss from batch 901 to 950 is 3.020314931869507
2018-04-11 02:28:03,009 - brc - INFO - Average loss from batch 951 to 1000 is 3.12787034034729
2018-04-11 02:29:07,799 - brc - INFO - Average loss from batch 1001 to 1050 is 3.0877183175086973
2018-04-11 02:30:12,769 - brc - INFO - Average loss from batch 1051 to 1100 is 3.059160008430481
2018-04-11 02:31:16,511 - brc - INFO - Average loss from batch 1101 to 1150 is 3.0313716650009157
2018-04-11 02:32:21,785 - brc - INFO - Average loss from batch 1151 to 1200 is 3.009136052131653
2018-04-11 02:33:27,449 - brc - INFO - Average loss from batch 1201 to 1250 is 2.941865978240967
2018-04-11 02:34:33,435 - brc - INFO - Average loss from batch 1251 to 1300 is 2.9953784227371214
2018-04-11 02:35:39,315 - brc - INFO - Average loss from batch 1301 to 1350 is 3.115221071243286
2018-04-11 02:36:44,810 - brc - INFO - Average loss from batch 1351 to 1400 is 2.9983146595954895
2018-04-11 02:37:50,591 - brc - INFO - Average loss from batch 1401 to 1450 is 3.1360720014572143
2018-04-11 02:38:56,516 - brc - INFO - Average loss from batch 1451 to 1500 is 3.056263291835785
2018-04-11 02:40:02,219 - brc - INFO - Average loss from batch 1501 to 1550 is 2.9031641244888307
2018-04-11 02:41:07,992 - brc - INFO - Average loss from batch 1551 to 1600 is 3.0057662177085875
2018-04-11 02:42:13,316 - brc - INFO - Average loss from batch 1601 to 1650 is 3.0822587537765505
2018-04-11 02:43:18,952 - brc - INFO - Average loss from batch 1651 to 1700 is 3.200595989227295
2018-04-11 02:44:24,912 - brc - INFO - Average loss from batch 1701 to 1750 is 3.224572877883911
2018-04-11 02:44:52,850 - brc - INFO - Average train loss for epoch 6 is 2.8986889738706805
2018-04-11 02:44:52,850 - brc - INFO - Evaluating the model after epoch 6
2018-04-11 02:50:04,659 - brc - INFO - Dev eval loss 18.955211560058594
2018-04-11 02:50:04,660 - brc - INFO - Dev eval result: {'Bleu-1': 0.382516895196655, 'Bleu-2': 0.31758018814916705, 'Bleu-3': 0.28001938854224556, 'Bleu-4': 0.2561115225423991, 'Rouge-L': 0.31246732818679973}
2018-04-11 02:50:04,660 - brc - INFO - Training the model for epoch 7
2018-04-11 02:51:10,070 - brc - INFO - Average loss from batch 1 to 50 is 2.041513488292694
2018-04-11 02:52:16,334 - brc - INFO - Average loss from batch 51 to 100 is 1.977960319519043
2018-04-11 02:53:22,055 - brc - INFO - Average loss from batch 101 to 150 is 2.017267651557922
2018-04-11 02:54:28,218 - brc - INFO - Average loss from batch 151 to 200 is 1.9130748105049133
2018-04-11 02:55:32,866 - brc - INFO - Average loss from batch 201 to 250 is 2.18491797208786
2018-04-11 02:56:38,988 - brc - INFO - Average loss from batch 251 to 300 is 2.0699531197547913
2018-04-11 02:57:45,030 - brc - INFO - Average loss from batch 301 to 350 is 2.1114244198799135
2018-04-11 02:58:50,977 - brc - INFO - Average loss from batch 351 to 400 is 2.0476448154449463
2018-04-11 02:59:56,978 - brc - INFO - Average loss from batch 401 to 450 is 2.2182854628562927
2018-04-11 03:01:03,057 - brc - INFO - Average loss from batch 451 to 500 is 2.1669406390190122
2018-04-11 03:02:09,026 - brc - INFO - Average loss from batch 501 to 550 is 2.132921533584595
2018-04-11 03:03:14,765 - brc - INFO - Average loss from batch 551 to 600 is 2.195620732307434
2018-04-11 03:04:20,643 - brc - INFO - Average loss from batch 601 to 650 is 2.184586379528046
2018-04-11 03:05:26,387 - brc - INFO - Average loss from batch 651 to 700 is 2.2487426137924196
2018-04-11 03:06:32,199 - brc - INFO - Average loss from batch 701 to 750 is 2.207325162887573
2018-04-11 03:07:36,611 - brc - INFO - Average loss from batch 751 to 800 is 2.2830372047424317
2018-04-11 03:08:43,755 - brc - INFO - Average loss from batch 801 to 850 is 2.3313030791282654
2018-04-11 03:09:49,292 - brc - INFO - Average loss from batch 851 to 900 is 2.3916206645965574
2018-04-11 03:10:55,143 - brc - INFO - Average loss from batch 901 to 950 is 2.3266694855690004
2018-04-11 03:12:00,692 - brc - INFO - Average loss from batch 951 to 1000 is 2.3970942187309263
2018-04-11 03:13:06,759 - brc - INFO - Average loss from batch 1001 to 1050 is 2.265863015651703
2018-04-11 03:14:12,747 - brc - INFO - Average loss from batch 1051 to 1100 is 2.401614320278168
2018-04-11 03:15:17,199 - brc - INFO - Average loss from batch 1101 to 1150 is 2.3610440945625304
2018-04-11 03:16:23,296 - brc - INFO - Average loss from batch 1151 to 1200 is 2.412475578784943
2018-04-11 03:17:29,242 - brc - INFO - Average loss from batch 1201 to 1250 is 2.373531758785248
2018-04-11 03:18:35,393 - brc - INFO - Average loss from batch 1251 to 1300 is 2.3952803301811216
2018-04-11 03:19:41,421 - brc - INFO - Average loss from batch 1301 to 1350 is 2.444842360019684
2018-04-11 03:20:47,302 - brc - INFO - Average loss from batch 1351 to 1400 is 2.3735543656349183
2018-04-11 03:21:53,423 - brc - INFO - Average loss from batch 1401 to 1450 is 2.5565956139564516
2018-04-11 03:22:59,301 - brc - INFO - Average loss from batch 1451 to 1500 is 2.46939510345459
2018-04-11 03:24:05,420 - brc - INFO - Average loss from batch 1501 to 1550 is 2.4329499197006226
2018-04-11 03:25:11,418 - brc - INFO - Average loss from batch 1551 to 1600 is 2.3198712491989135
2018-04-11 03:26:16,065 - brc - INFO - Average loss from batch 1601 to 1650 is 2.380464262962341
2018-04-11 03:27:23,307 - brc - INFO - Average loss from batch 1651 to 1700 is 2.465847361087799
2018-04-11 03:28:27,878 - brc - INFO - Average loss from batch 1701 to 1750 is 2.6002350735664366
2018-04-11 03:28:58,302 - brc - INFO - Average train loss for epoch 7 is 2.278646725263017
2018-04-11 03:28:58,302 - brc - INFO - Evaluating the model after epoch 7
2018-04-11 03:34:17,969 - brc - INFO - Dev eval loss 20.27416611328125
2018-04-11 03:34:17,969 - brc - INFO - Dev eval result: {'Bleu-1': 0.43821476657827724, 'Bleu-2': 0.36262163543367737, 'Bleu-3': 0.31884797448570545, 'Bleu-4': 0.2909531950232703, 'Rouge-L': 0.31824628908440666}
2018-04-11 03:34:17,969 - brc - INFO - Training the model for epoch 8
2018-04-11 03:35:23,341 - brc - INFO - Average loss from batch 1 to 50 is 1.5856313824653625
2018-04-11 03:36:29,175 - brc - INFO - Average loss from batch 51 to 100 is 1.4280942583084106
2018-04-11 03:37:34,923 - brc - INFO - Average loss from batch 101 to 150 is 1.6404920864105224
2018-04-11 03:38:40,461 - brc - INFO - Average loss from batch 151 to 200 is 1.5694440722465515
2018-04-11 03:39:46,255 - brc - INFO - Average loss from batch 201 to 250 is 1.621625063419342
2018-04-11 03:40:52,059 - brc - INFO - Average loss from batch 251 to 300 is 1.667514990568161
2018-04-11 03:41:57,563 - brc - INFO - Average loss from batch 301 to 350 is 1.6433900451660157
2018-04-11 03:43:02,329 - brc - INFO - Average loss from batch 351 to 400 is 1.6722470450401306
2018-04-11 03:44:07,361 - brc - INFO - Average loss from batch 401 to 450 is 1.6854212164878846
2018-04-11 03:45:11,030 - brc - INFO - Average loss from batch 451 to 500 is 1.60212810754776
2018-04-11 03:46:16,227 - brc - INFO - Average loss from batch 501 to 550 is 1.699009782075882
2018-04-11 03:47:21,325 - brc - INFO - Average loss from batch 551 to 600 is 1.6732561659812928
2018-04-11 03:48:26,273 - brc - INFO - Average loss from batch 601 to 650 is 1.6645222401618958
2018-04-11 03:49:31,280 - brc - INFO - Average loss from batch 651 to 700 is 1.7840521550178527
2018-04-11 03:50:36,349 - brc - INFO - Average loss from batch 701 to 750 is 1.7338312780857086
2018-04-11 03:51:40,652 - brc - INFO - Average loss from batch 751 to 800 is 1.872095365524292
2018-04-11 03:52:44,697 - brc - INFO - Average loss from batch 801 to 850 is 1.6669320809841155
2018-04-11 03:53:48,904 - brc - INFO - Average loss from batch 851 to 900 is 1.8241050267219543
2018-04-11 03:54:52,777 - brc - INFO - Average loss from batch 901 to 950 is 1.7808349561691283
2018-04-11 03:55:56,940 - brc - INFO - Average loss from batch 951 to 1000 is 1.868481446504593
2018-04-11 03:56:59,669 - brc - INFO - Average loss from batch 1001 to 1050 is 1.9272246217727662
2018-04-11 03:58:03,717 - brc - INFO - Average loss from batch 1051 to 1100 is 1.9422909092903138
2018-04-11 03:59:07,883 - brc - INFO - Average loss from batch 1101 to 1150 is 1.9578941130638123
2018-04-11 04:00:11,963 - brc - INFO - Average loss from batch 1151 to 1200 is 2.0029760229587557
2018-04-11 04:01:16,068 - brc - INFO - Average loss from batch 1201 to 1250 is 1.9004557728767395
2018-04-11 04:02:20,165 - brc - INFO - Average loss from batch 1251 to 1300 is 1.9443702602386475
2018-04-11 04:03:24,079 - brc - INFO - Average loss from batch 1301 to 1350 is 1.9753352308273315
2018-04-11 04:04:28,040 - brc - INFO - Average loss from batch 1351 to 1400 is 1.8670297408103942
2018-04-11 04:05:30,489 - brc - INFO - Average loss from batch 1401 to 1450 is 1.898145751953125
2018-04-11 04:06:34,218 - brc - INFO - Average loss from batch 1451 to 1500 is 2.0044447386264803
2018-04-11 04:07:37,839 - brc - INFO - Average loss from batch 1501 to 1550 is 2.0196635341644287
2018-04-11 04:08:41,530 - brc - INFO - Average loss from batch 1551 to 1600 is 2.0346659886837006
2018-04-11 04:09:45,212 - brc - INFO - Average loss from batch 1601 to 1650 is 2.02316285610199
2018-04-11 04:10:48,766 - brc - INFO - Average loss from batch 1651 to 1700 is 2.0076228201389315
2018-04-11 04:11:52,521 - brc - INFO - Average loss from batch 1701 to 1750 is 2.015536890029907
2018-04-11 04:12:20,639 - brc - INFO - Average train loss for epoch 8 is 1.8074468703565905
2018-04-11 04:12:20,639 - brc - INFO - Evaluating the model after epoch 8
2018-04-11 04:17:24,938 - brc - INFO - Dev eval loss 22.5215395904541
2018-04-11 04:17:24,938 - brc - INFO - Dev eval result: {'Bleu-1': 0.3686872650229446, 'Bleu-2': 0.30452752634431896, 'Bleu-3': 0.2673897280361072, 'Bleu-4': 0.24381814017534226, 'Rouge-L': 0.29429035565458583}
2018-04-11 04:17:24,938 - brc - INFO - Training the model for epoch 9
2018-04-11 04:18:27,188 - brc - INFO - Average loss from batch 1 to 50 is 1.2533151292800904
2018-04-11 04:19:31,269 - brc - INFO - Average loss from batch 51 to 100 is 1.2131905031204224
2018-04-11 04:20:35,508 - brc - INFO - Average loss from batch 101 to 150 is 1.150348323583603
2018-04-11 04:21:39,729 - brc - INFO - Average loss from batch 151 to 200 is 1.293256402015686
2018-04-11 04:22:43,663 - brc - INFO - Average loss from batch 201 to 250 is 1.289431699514389
2018-04-11 04:23:47,707 - brc - INFO - Average loss from batch 251 to 300 is 1.2922159785032272
2018-04-11 04:24:51,804 - brc - INFO - Average loss from batch 301 to 350 is 1.2192641770839692
2018-04-11 04:25:55,996 - brc - INFO - Average loss from batch 351 to 400 is 1.4090255844593047
2018-04-11 04:27:00,482 - brc - INFO - Average loss from batch 401 to 450 is 1.3824343359470368
2018-04-11 04:28:04,179 - brc - INFO - Average loss from batch 451 to 500 is 1.3486550700664521
2018-04-11 04:29:09,128 - brc - INFO - Average loss from batch 501 to 550 is 1.3961688697338104
2018-04-11 04:30:14,253 - brc - INFO - Average loss from batch 551 to 600 is 1.4521543192863464
2018-04-11 04:31:19,417 - brc - INFO - Average loss from batch 601 to 650 is 1.308864622116089
2018-04-11 04:32:24,290 - brc - INFO - Average loss from batch 651 to 700 is 1.3768998789787292
2018-04-11 04:33:29,301 - brc - INFO - Average loss from batch 701 to 750 is 1.435883333683014
2018-04-11 04:34:33,931 - brc - INFO - Average loss from batch 751 to 800 is 1.3270233178138733
2018-04-11 04:35:39,132 - brc - INFO - Average loss from batch 801 to 850 is 1.4739203071594238
2018-04-11 04:36:43,782 - brc - INFO - Average loss from batch 851 to 900 is 1.4973648166656495
2018-04-11 04:37:48,349 - brc - INFO - Average loss from batch 901 to 950 is 1.4621024894714356
2018-04-11 04:38:52,983 - brc - INFO - Average loss from batch 951 to 1000 is 1.393100655078888
2018-04-11 04:39:57,600 - brc - INFO - Average loss from batch 1001 to 1050 is 1.534791705608368
2018-04-11 04:41:00,844 - brc - INFO - Average loss from batch 1051 to 1100 is 1.3894280195236206
2018-04-11 04:42:05,448 - brc - INFO - Average loss from batch 1101 to 1150 is 1.373667551279068
2018-04-11 04:43:10,057 - brc - INFO - Average loss from batch 1151 to 1200 is 1.591466040611267
2018-04-11 04:44:14,810 - brc - INFO - Average loss from batch 1201 to 1250 is 1.4953562128543854
2018-04-11 04:45:19,505 - brc - INFO - Average loss from batch 1251 to 1300 is 1.5627141189575195
2018-04-11 04:46:24,469 - brc - INFO - Average loss from batch 1301 to 1350 is 1.5944009184837342
2018-04-11 04:47:29,385 - brc - INFO - Average loss from batch 1351 to 1400 is 1.4606982851028443
2018-04-11 04:48:34,254 - brc - INFO - Average loss from batch 1401 to 1450 is 1.5990923893451692
2018-04-11 04:49:39,364 - brc - INFO - Average loss from batch 1451 to 1500 is 1.5792182302474975
2018-04-11 04:50:44,419 - brc - INFO - Average loss from batch 1501 to 1550 is 1.6172304832935334
2018-04-11 04:51:49,305 - brc - INFO - Average loss from batch 1551 to 1600 is 1.6299488687515258
2018-04-11 04:52:52,802 - brc - INFO - Average loss from batch 1601 to 1650 is 1.62121408700943
2018-04-11 04:53:57,893 - brc - INFO - Average loss from batch 1651 to 1700 is 1.68129718542099
2018-04-11 04:55:03,035 - brc - INFO - Average loss from batch 1701 to 1750 is 1.8702539086341858
2018-04-11 04:55:31,756 - brc - INFO - Average train loss for epoch 9 is 1.4473563463537065
2018-04-11 04:55:31,756 - brc - INFO - Evaluating the model after epoch 9
2018-04-11 05:00:46,066 - brc - INFO - Dev eval loss 23.363765130615235
2018-04-11 05:00:46,066 - brc - INFO - Dev eval result: {'Bleu-1': 0.3927714757865321, 'Bleu-2': 0.32335487943658725, 'Bleu-3': 0.28349542349039125, 'Bleu-4': 0.2582967275560385, 'Rouge-L': 0.29519198454379209}
2018-04-11 05:00:46,066 - brc - INFO - Training the model for epoch 10
2018-04-11 05:01:50,492 - brc - INFO - Average loss from batch 1 to 50 is 1.0161880522966384
2018-04-11 05:02:55,331 - brc - INFO - Average loss from batch 51 to 100 is 0.9246705448627472
2018-04-11 05:04:00,313 - brc - INFO - Average loss from batch 101 to 150 is 0.9466960233449936
2018-04-11 05:05:05,099 - brc - INFO - Average loss from batch 151 to 200 is 0.9556963557004928
2018-04-11 05:06:10,045 - brc - INFO - Average loss from batch 201 to 250 is 1.0249802231788636
2018-04-11 05:07:15,162 - brc - INFO - Average loss from batch 251 to 300 is 1.0429013454914093
2018-04-11 05:08:19,672 - brc - INFO - Average loss from batch 301 to 350 is 1.0541857022047043
2018-04-11 05:09:24,385 - brc - INFO - Average loss from batch 351 to 400 is 1.1255643224716188
2018-04-11 05:10:27,745 - brc - INFO - Average loss from batch 401 to 450 is 1.0392792057991027
2018-04-11 05:11:33,750 - brc - INFO - Average loss from batch 451 to 500 is 1.0440021574497222
2018-04-11 05:12:37,274 - brc - INFO - Average loss from batch 501 to 550 is 1.1154374051094056
2018-04-11 05:13:42,151 - brc - INFO - Average loss from batch 551 to 600 is 1.1977213382720948
2018-04-11 05:14:46,542 - brc - INFO - Average loss from batch 601 to 650 is 1.1467753338813782
2018-04-11 05:15:51,127 - brc - INFO - Average loss from batch 651 to 700 is 1.176077418923378
2018-04-11 05:16:55,749 - brc - INFO - Average loss from batch 701 to 750 is 1.16417151927948
2018-04-11 05:18:00,310 - brc - INFO - Average loss from batch 751 to 800 is 1.0815985417366027
2018-04-11 05:19:05,110 - brc - INFO - Average loss from batch 801 to 850 is 1.204755378961563
2018-04-11 05:20:09,806 - brc - INFO - Average loss from batch 851 to 900 is 1.2324313938617706
2018-04-11 05:21:14,391 - brc - INFO - Average loss from batch 901 to 950 is 1.1811530923843383
2018-04-11 05:22:19,178 - brc - INFO - Average loss from batch 951 to 1000 is 1.2318221020698548
2018-04-11 05:23:23,776 - brc - INFO - Average loss from batch 1001 to 1050 is 1.2501273477077484
2018-04-11 05:24:27,019 - brc - INFO - Average loss from batch 1051 to 1100 is 1.2086901724338532
2018-04-11 05:25:31,580 - brc - INFO - Average loss from batch 1101 to 1150 is 1.4252093315124512
2018-04-11 05:26:36,383 - brc - INFO - Average loss from batch 1151 to 1200 is 1.1947151911258698
2018-04-11 05:27:40,959 - brc - INFO - Average loss from batch 1201 to 1250 is 1.2817790150642394
2018-04-11 05:28:45,362 - brc - INFO - Average loss from batch 1251 to 1300 is 1.3593522357940673
2018-04-11 05:29:50,141 - brc - INFO - Average loss from batch 1301 to 1350 is 1.3051958733797073
2018-04-11 05:30:54,887 - brc - INFO - Average loss from batch 1351 to 1400 is 1.2854891657829284
2018-04-11 05:31:59,759 - brc - INFO - Average loss from batch 1401 to 1450 is 1.2880143058300018
2018-04-11 05:33:04,605 - brc - INFO - Average loss from batch 1451 to 1500 is 1.1516368365287781
2018-04-11 05:34:09,268 - brc - INFO - Average loss from batch 1501 to 1550 is 1.3379163509607315
2018-04-11 05:35:13,913 - brc - INFO - Average loss from batch 1551 to 1600 is 1.3333748400211334
2018-04-11 05:36:17,384 - brc - INFO - Average loss from batch 1601 to 1650 is 1.2987282836437226
2018-04-11 05:37:22,330 - brc - INFO - Average loss from batch 1651 to 1700 is 1.3622823774814605
2018-04-11 05:38:27,268 - brc - INFO - Average loss from batch 1701 to 1750 is 1.3543532037734984
2018-04-11 05:38:56,197 - brc - INFO - Average train loss for epoch 10 is 1.183787075448775
2018-04-11 05:38:56,197 - brc - INFO - Evaluating the model after epoch 10
2018-04-11 05:44:03,264 - brc - INFO - Dev eval loss 25.90659103393555
2018-04-11 05:44:03,264 - brc - INFO - Dev eval result: {'Bleu-1': 0.36600754742492214, 'Bleu-2': 0.3007295205640707, 'Bleu-3': 0.2630229578744172, 'Bleu-4': 0.2392085398087568, 'Rouge-L': 0.28995081609848966}
2018-04-11 05:44:03,264 - brc - INFO - Done with model training!
{'testlen': 607089, 'reflen': 790002, 'guess': [607089, 597652, 588227, 578863], 'correct': [330756, 225456, 182666, 161443]}
ratio: 0.7684651431262189
{'testlen': 919083, 'reflen': 880731, 'guess': [919083, 909646, 900220, 890846], 'correct': [457905, 320349, 264989, 237109]}
ratio: 1.0435456456057512
{'testlen': 547408, 'reflen': 771761, 'guess': [547408, 537971, 528592, 519296], 'correct': [306082, 210556, 171004, 151014]}
ratio: 0.7092973083635986
{'testlen': 733690, 'reflen': 824874, 'guess': [733690, 724253, 714963, 705737], 'correct': [374239, 255907, 208485, 184965]}
ratio: 0.889457056471654
{'testlen': 744557, 'reflen': 826958, 'guess': [744557, 735120, 725782, 716495], 'correct': [377663, 257312, 209694, 186095]}
ratio: 0.9003564872702109
{'testlen': 590179, 'reflen': 787444, 'guess': [590179, 580742, 571434, 562208], 'correct': [315352, 213895, 173774, 153890]}
ratio: 0.74948694764326
{'testlen': 723264, 'reflen': 822957, 'guess': [723264, 713827, 704502, 695259], 'correct': [363786, 245853, 199337, 176421]}
ratio: 0.8788600133421297
{'testlen': 581035, 'reflen': 785295, 'guess': [581035, 571598, 562395, 553370], 'correct': [304463, 204343, 164777, 145385]}
ratio: 0.7398939252128172
{'testlen': 637641, 'reflen': 797920, 'guess': [637641, 628204, 618895, 609745], 'correct': [322019, 215023, 173405, 153162]}
ratio: 0.79912898536194
{'testlen': 576588, 'reflen': 779898, 'guess': [576588, 567151, 557879, 548781], 'correct': [300256, 199387, 159700, 140495]}
ratio: 0.7393120638852763
2018-04-11 05:44:08,889 - brc - INFO - Running with args : Namespace(algo='BIDAF', batch_size=32, brc_dir='../data/baidu', dev_files=['../data/preprocessed/devset/search.dev.json', '../data/preprocessed/devset/zhidao.dev.json'], dropout_keep_prob=1, embed_size=300, epochs=10, evaluate=True, gpu='2', hidden_size=150, learning_rate=0.001, log_path=None, max_a_len=200, max_p_len=500, max_p_num=5, max_q_len=60, model_dir='../data/models/', optim='adam', predict=False, prepare=False, result_dir='../data/results/', summary_dir='../data/summary/', test_files=['../data/preprocessed/testset/search.test1.json', '../data/preprocessed/testset/zhidao.test1.json'], train=False, train_files=['../data/preprocessed/trainset/search.train.json', '../data/preprocessed/trainset/zhidao.train.json'], vocab_dir='../data/vocab/', weight_decay=0)
2018-04-11 05:44:08,890 - brc - INFO - Load data_set and vocab...
2018-04-11 05:44:20,348 - brc - INFO - Dev set size: 10000 questions.
2018-04-11 05:44:20,348 - brc - INFO - Converting text into ids...
2018-04-11 05:44:22,480 - brc - INFO - Restoring the model...
2018-04-11 05:44:26,740 - brc - INFO - Time to build graph: 2.5936942100524902 s
2018-04-11 05:44:31,326 - brc - INFO - There are 77168403 parameters in the model
2018-04-11 05:44:33,286 - brc - INFO - Model restored from ../data/models/, with prefix BIDAF
2018-04-11 05:44:33,287 - brc - INFO - Evaluating the model on dev set...
2018-04-11 05:48:34,153 - brc - INFO - Saving dev.predicted results to ../data/results/dev.predicted.json
2018-04-11 05:50:07,404 - brc - INFO - Loss on dev set: 12.993387455749511
2018-04-11 05:50:07,404 - brc - INFO - Result on dev set: {'Bleu-1': 0.4982194208792889, 'Bleu-2': 0.4188763162510996, 'Bleu-3': 0.372406226253392, 'Bleu-4': 0.3424120589823242, 'Rouge-L': 0.37101418869658664}
2018-04-11 05:50:07,404 - brc - INFO - Predicted answers are saved to ../data/results/
{'testlen': 919083, 'reflen': 880731, 'guess': [919083, 909646, 900220, 890846], 'correct': [457905, 320349, 264989, 237109]}
ratio: 1.0435456456057512
2018-04-11 05:50:09,703 - brc - INFO - Running with args : Namespace(algo='BIDAF', batch_size=32, brc_dir='../data/baidu', dev_files=['../data/preprocessed/devset/search.dev.json', '../data/preprocessed/devset/zhidao.dev.json'], dropout_keep_prob=1, embed_size=300, epochs=10, evaluate=False, gpu='2', hidden_size=150, learning_rate=0.001, log_path=None, max_a_len=200, max_p_len=500, max_p_num=5, max_q_len=60, model_dir='../data/models/', optim='adam', predict=True, prepare=False, result_dir='../data/results/', summary_dir='../data/summary/', test_files=['../data/preprocessed/testset/search.test1.json', '../data/preprocessed/testset/zhidao.test1.json'], train=False, train_files=['../data/preprocessed/trainset/search.train.json', '../data/preprocessed/trainset/zhidao.train.json'], vocab_dir='../data/vocab/', weight_decay=0)
2018-04-11 05:50:09,703 - brc - INFO - Load data_set and vocab...
2018-04-11 05:51:10,979 - brc - INFO - Test set size: 60000 questions.
2018-04-11 05:51:10,979 - brc - INFO - Converting text into ids...
2018-04-11 05:51:24,316 - brc - INFO - Restoring the model...
2018-04-11 05:51:31,328 - brc - INFO - Time to build graph: 5.3300886154174805 s
2018-04-11 05:51:37,246 - brc - INFO - There are 77168403 parameters in the model
2018-04-11 05:51:39,218 - brc - INFO - Model restored from ../data/models/, with prefix BIDAF
2018-04-11 05:51:39,218 - brc - INFO - Predicting answers for test set...
2018-04-11 06:15:27,353 - brc - INFO - Saving test.predicted results to ../data/results/test.predicted.json
